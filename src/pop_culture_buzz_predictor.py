# -*- coding: utf-8 -*-
"""Pop Culture Buzz Predictor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a2dIyWbfQsdOlN8y6AvCVO2kXphkLmjK
"""

!pip install pytrends prophet plotly pandas numpy matplotlib

import os
import time
import numpy as np
import pandas as pd
from pytrends.request import TrendReq

# -----------------------------
# Config
# -----------------------------
os.makedirs("data/raw", exist_ok=True)
pytrends = TrendReq(hl="en-US", tz=360)

anchor = "Instagram"  # use a strong but not overwhelming anchor
keywords = [
    'Rihanna', 'Kendrick Lamar', 'Taylor Swift', 'Travis Scott', 'BTS',
    'Blackpink', 'Doja Cat', 'Olivia Rodrigo', 'Zendaya', 'Jenna Ortega',
    'Wednesday', 'Stranger Things'
]

TIMEFRAME = "today 12-m"  # recent buzz
SLEEP_BETWEEN = 12        # be polite to Google; increase if you hit errors
RETRIES = 3

def fetch_pair_ratio(kw, anchor, retries=RETRIES, sleep_time=SLEEP_BETWEEN):
    """
    Fetch keyword+anchor from Google Trends and return a Series of
    100 * (kw / anchor) per date, with safe zero handling.
    """
    for attempt in range(1, retries+1):
        try:
            pytrends.build_payload([kw, anchor], timeframe=TIMEFRAME, geo="")
            df = pytrends.interest_over_time().drop(columns=["isPartial"], errors="ignore")
            if df.empty:
                print(f"⚠️ Empty data for {kw}")
                return None

            # Per-date ratio: 100 * kw / anchor
            denom = df[anchor].replace(0, np.nan)
            ratio = 100.0 * (df[kw] / denom)

            # Handle NaNs from zero anchor: fill by interpolation, then zeros
            ratio = ratio.interpolate(limit_direction="both").fillna(0.0)

            ratio.name = kw
            return ratio

        except Exception as e:
            print(f"❌ Error fetching {kw} (attempt {attempt}/{retries}): {e}")
            time.sleep(sleep_time * attempt)  # backoff
    return None

# -----------------------------
# Collect all series
# -----------------------------
series_list = []
for kw in keywords:
    print(f"\nFetching {kw} + {anchor} ({TIMEFRAME})")
    s = fetch_pair_ratio(kw, anchor)
    if s is not None:
        series_list.append(s)
    time.sleep(SLEEP_BETWEEN)

if not series_list:
    raise SystemExit("❌ No data collected. Try increasing SLEEP_BETWEEN or changing TIMEFRAME/anchor.")

# Align on dates and combine
combined = pd.concat(series_list, axis=1)
combined.index.name = "date"

# -----------------------------
# Global rescale to 0–100
# -----------------------------
max_val = combined.to_numpy().max()
if max_val and max_val > 0:
    scale = 100.0 / max_val
    combined_scaled = combined * scale
else:
    combined_scaled = combined.copy()

# Save
out_path = "data/raw/google_trends_anchor_scaled.csv"
combined_scaled.reset_index().to_csv(out_path, index=False)
print(f"\n✅ Saved comparable series to: {out_path}")
print("Shape:", combined_scaled.shape)
print(combined_scaled.head())

import pandas as pd
import numpy as np

# Load uploaded CSV
df = pd.read_csv("google_trends_anchor_scaled.csv", parse_dates=["date"])

# Clean: replace zeros with NaN, interpolate, smooth with 7-day rolling mean
df_clean = (
    df.set_index("date")
      .replace(0, np.nan)
      .interpolate(method="linear", limit_direction="both")
      .rolling(window=7, min_periods=1)
      .mean()
      .reset_index()
)

# Save cleaned file
out_path = "google_trends_clean.csv"
df_clean.to_csv(out_path, index=False)

print(f"✅ Cleaned data saved: {out_path}")
print("Shape:", df_clean.shape)
print(df_clean.head())

import pandas as pd
import numpy as np

# -----------------------------
# Load CSV
# -----------------------------
df = pd.read_csv("google_trends_anchor_scaled.csv")

# Ensure 'date' column is datetime
df['date'] = pd.to_datetime(df['date'], errors='coerce')
df = df.dropna(subset=['date'])

# Sort and remove duplicate dates
df = df.sort_values('date')
df = df.drop_duplicates(subset='date', keep='first')
df.set_index('date', inplace=True)

# Ensure numeric columns
df = df.apply(pd.to_numeric, errors='coerce')
df.replace(0, np.nan, inplace=True)
df.dropna(axis=1, how='all', inplace=True)

# Convert index to numeric days
df_numeric = df.copy()
df_numeric.index = (df_numeric.index - df_numeric.index[0]).days

# -----------------------------
# Spline interpolation per column
# -----------------------------
df_interp_numeric = pd.DataFrame(index=df_numeric.index)

for col in df_numeric.columns:
    valid_points = df_numeric[col].dropna()
    if len(valid_points) >= 3:  # enough points for order=2 spline
        df_interp_numeric[col] = df_numeric[col].interpolate(method='spline', order=2, limit_direction='both')
    else:
        df_interp_numeric[col] = df_numeric[col].interpolate(method='linear', limit_direction='both')

# Restore datetime index
df_interp = df_interp_numeric.copy()
df_interp.index = df.index

# Fill any remaining NaNs
df_interp.fillna(method='ffill', inplace=True)
df_interp.fillna(method='bfill', inplace=True)

# -----------------------------
# Short-term and long-term smoothing
# -----------------------------
df_short = df_interp.rolling(window=14, min_periods=1).mean()  # 2-week rolling mean
df_long  = df_interp.rolling(window=49, min_periods=1).mean()  # 7-week rolling mean

# -----------------------------
# Spike detection
# -----------------------------
df_short = df_short.apply(pd.to_numeric, errors='coerce')
df_long  = df_long.apply(pd.to_numeric, errors='coerce')
spikes = df_short - df_long

# Dynamic threshold: mean + 2*std per column
spike_flags = spikes.gt(spikes.mean() + 2*spikes.std())

# -----------------------------
# Save outputs
# -----------------------------
df_short.reset_index().to_csv("google_trends_short_term.csv", index=False)
df_long.reset_index().to_csv("google_trends_long_term.csv", index=False)
spikes.reset_index().to_csv("google_trends_spikes.csv", index=False)
spike_flags.reset_index().to_csv("google_trends_spike_flags.csv", index=False)

print("✅ Short-term, long-term, spikes, and spike flags saved successfully!")

import matplotlib.pyplot as plt

# Set bigger figure
plt.figure(figsize=(18,8))  # wider and taller

# Plot short-term and long-term separately for clarity
for col in df_short.columns:
    plt.plot(df_short.index, df_short[col], label=col)
    plt.plot(df_long.index, df_long[col], linestyle='--', alpha=0.7)

plt.title("Buzz Over Time (Short-term vs Long-term)", fontsize=16)
plt.xlabel("Date", fontsize=14)
plt.ylabel("Buzz (Scaled)", fontsize=14)

# Make legend outside the plot
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=12)

plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Load cleaned data
df_clean = pd.read_csv("google_trends_clean.csv", parse_dates=["date"])

# Ensure 'date' is datetime and set as index
df_clean['date'] = pd.to_datetime(df_clean['date'], errors='coerce')
df_clean = df_clean.dropna(subset=['date'])
df_clean = df_clean.sort_values('date')
df_clean.set_index('date', inplace=True)

# Ensure all columns are numeric
for col in df_clean.columns:
    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')

# Replace zeros with NaN (optional)
df_clean.replace(0, np.nan, inplace=True)

# Resample weekly and fill missing values
df_weekly = df_clean.resample('W').mean()
df_weekly = df_weekly.fillna(method='ffill').fillna(method='bfill')

# Plot weekly buzz
plt.figure(figsize=(18,8))
for col in df_weekly.columns:
    plt.plot(df_weekly.index, df_weekly[col], label=col)

plt.title("Weekly Buzz Over Time", fontsize=16)
plt.xlabel("Week", fontsize=14)
plt.ylabel("Buzz (Scaled)", fontsize=14)

# Legend outside plot
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=12)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load cleaned data
df_clean = pd.read_csv("google_trends_clean.csv", parse_dates=["date"])
df_clean['date'] = pd.to_datetime(df_clean['date'], errors='coerce')
df_clean = df_clean.dropna(subset=['date'])
df_clean = df_clean.sort_values('date')
df_clean.set_index('date', inplace=True)

# Ensure all columns numeric
for col in df_clean.columns:
    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')

# Replace zeros with NaN (optional)
df_clean.replace(0, np.nan, inplace=True)

# Resample weekly
df_weekly = df_clean.resample('W').mean()
df_weekly = df_weekly.fillna(method='ffill').fillna(method='bfill')

# -----------------------------
# Correlation matrix
# -----------------------------
corr = df_weekly.corr()  # Pearson correlation by default

# -----------------------------
# Plot heatmap
# -----------------------------
plt.figure(figsize=(12,10))
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", cbar=True, square=True, linewidths=0.5)
plt.title("Correlation Heatmap of Weekly Buzz", fontsize=16)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Load weekly Google Trends data
df = pd.read_csv("google_trends_clean.csv", parse_dates=["date"])
df.set_index("date", inplace=True)

# Convert to relative % share directly (since it's already weekly)
df_relative = df.div(df.sum(axis=1), axis=0) * 100

# Plot stacked area chart
plt.figure(figsize=(12,6))
df_relative.plot.area(alpha=0.7, figsize=(12,6))
plt.title("Relative Buzz Share Over Time (Weekly, %)", fontsize=14)
plt.ylabel("Relative Share (%)")
plt.xlabel("Week")
plt.legend(title="Keywords", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

# -----------------------------
# Load spike flags
# -----------------------------
df_spikes = pd.read_csv("google_trends_spike_flags.csv", parse_dates=["date"])
df_spikes.set_index("date", inplace=True)

# -----------------------------
# Generate distinct colors
# -----------------------------
n = len(df_spikes.columns)
colors = cm.get_cmap("tab10", n)  # "tab20" = 20 distinct colors

# -----------------------------
# Plot grouped scatter with colors
# -----------------------------
plt.figure(figsize=(12,6))

for i, col in enumerate(df_spikes.columns):
    spike_dates = df_spikes.index[df_spikes[col] == True]
    plt.scatter(spike_dates, [col]*len(spike_dates),
                marker="*", s=150, color=colors(i), label=col)

plt.title("Buzz Spike Events (⚡)", fontsize=14)
plt.xlabel("Date")
plt.ylabel("Artist / Keyword")
plt.xticks(rotation=45)
plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.show()

!pip install prophet pandas matplotlib

import os
import pandas as pd
import matplotlib.pyplot as plt
from prophet import Prophet

# -----------------------------
# Config
# -----------------------------
CLEAN_CSV = "google_trends_clean.csv"
FORECAST_WEEKS = 12    # change horizon here
OUT_DIR = "forecasts_prophet"
os.makedirs(OUT_DIR, exist_ok=True)

# -----------------------------
# Load + prepare weekly data
# -----------------------------
df = pd.read_csv(CLEAN_CSV)

# Ensure datetime type
df["date"] = pd.to_datetime(df["date"], errors="coerce")
df = df.dropna(subset=["date"])
df = df.set_index("date")

# Keep numeric only
df = df.apply(pd.to_numeric, errors="coerce")

# Resample weekly
dfW = df.resample("W").mean().ffill().bfill()

print("✅ Data prepared:", dfW.shape, "rows,", dfW.shape[1], "series")

# -----------------------------
# Forecast function
# -----------------------------
def forecast_artist(series_name, horizon=12):
    s = dfW[[series_name]].copy()
    s = s.rename(columns={series_name: "y"})
    s["ds"] = s.index

    # Prophet expects non-negative
    s["y"] = s["y"].clip(lower=0)

    # Build & fit model
    m = Prophet(
        weekly_seasonality=True,
        yearly_seasonality=False,
        daily_seasonality=False,
        changepoint_prior_scale=0.5
    )
    m.fit(s)

    # Future dates
    future = m.make_future_dataframe(periods=horizon, freq="W")
    fcst = m.predict(future)

    # Save forecast CSV
    out_csv = os.path.join(OUT_DIR, f"{series_name.replace(' ', '_')}_forecast.csv")
    fcst[["ds", "yhat", "yhat_lower", "yhat_upper"]].to_csv(out_csv, index=False)

    # Plot
    plt.figure(figsize=(12,6))
    plt.plot(s["ds"], s["y"], label="history")
    plt.plot(fcst["ds"], fcst["yhat"], linestyle="--", label="forecast")
    plt.fill_between(fcst["ds"], fcst["yhat_lower"], fcst["yhat_upper"], alpha=0.2, label="uncertainty")
    plt.title(f"{series_name} — {horizon}-week Forecast (Prophet)")
    plt.xlabel("Week")
    plt.ylabel("Buzz (scaled)")
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()

    out_png = os.path.join(OUT_DIR, f"{series_name.replace(' ', '_')}_forecast.png")
    plt.savefig(out_png, dpi=150)
    plt.close()

    print(f"✅ {series_name}: saved {out_csv}, {out_png}")

    return fcst

# -----------------------------
# Run forecasts for all artists
# -----------------------------
all_forecasts = {}
for col in dfW.columns:
    try:
        fcst = forecast_artist(col, horizon=FORECAST_WEEKS)
        all_forecasts[col] = fcst
    except Exception as e:
        print(f"⚠️ Skipped {col}: {e}")

print(f"\n🎉 Forecasting complete. Results saved in: {OUT_DIR}")

!pip install streamlit pyngrok plotly prophet

# Launch Streamlit in Colab
!pip install streamlit_jupyter  # helper for Colab

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import plotly.express as px
# from prophet import Prophet
# 
# # -----------------------------
# # Load data safely with dayfirst
# # -----------------------------
# df = pd.read_csv("google_trends_clean.csv")
# df["date"] = pd.to_datetime(df["date"], dayfirst=True, errors="coerce")
# 
# df_flags = pd.read_csv("google_trends_spike_flags.csv")
# df_flags["date"] = pd.to_datetime(df_flags["date"], dayfirst=True, errors="coerce")
# 
# # Drop bad rows if any
# df = df.dropna(subset=["date"])
# df_flags = df_flags.dropna(subset=["date"])
# 
# artists = [c for c in df.columns if c != "date"]
# 
# # -----------------------------
# # Streamlit setup
# # -----------------------------
# st.set_page_config(page_title="PopBuzz Dashboard", layout="wide")
# st.title("📊 Pop Culture Buzz Predictor")
# 
# # Sidebar controls
# artist = st.sidebar.selectbox("Choose artist/show", artists)
# horizon = st.sidebar.slider("Forecast horizon (weeks)", 4, 52, 12)
# show_share = st.sidebar.checkbox("Show Relative Buzz Share (%)", value=False)
# 
# # Sidebar conclusion box
# conclusion = st.sidebar.text_area("📝 Conclusion (summary)", "Add final thoughts here...")
# 
# # -----------------------------
# # Historical + spikes
# # -----------------------------
# fig_hist = px.line(df, x="date", y=artist, title=f"Buzz Over Time — {artist}")
# spikes = df_flags[df_flags[artist] == True]
# fig_hist.add_scatter(
#     x=spikes["date"],
#     y=df.loc[df["date"].isin(spikes["date"]), artist],
#     mode="markers",
#     marker=dict(size=10, color="red", symbol="star"),
#     name="Spikes ⚡"
# )
# st.plotly_chart(fig_hist, use_container_width=True)
# 
# # -----------------------------
# # Forecast with Prophet
# # -----------------------------
# s = df[["date", artist]].rename(columns={"date": "ds", artist: "y"})
# s["ds"] = pd.to_datetime(s["ds"], dayfirst=True, errors="coerce")
# s = s.dropna(subset=["ds"])
# s["y"] = s["y"].clip(lower=0)
# 
# m = Prophet(weekly_seasonality=True, yearly_seasonality=False, changepoint_prior_scale=0.5)
# m.fit(s)
# 
# future = m.make_future_dataframe(periods=horizon, freq="W")
# fcst = m.predict(future)
# 
# fig_fcst = px.line(fcst, x="ds", y="yhat", title=f"Forecast — {artist} ({horizon} weeks)")
# fig_fcst.add_scatter(x=s["ds"], y=s["y"], mode="lines", name="History")
# fig_fcst.add_scatter(x=fcst["ds"], y=fcst["yhat_lower"], mode="lines", line=dict(dash="dot"), name="Lower Bound")
# fig_fcst.add_scatter(x=fcst["ds"], y=fcst["yhat_upper"], mode="lines", line=dict(dash="dot"), name="Upper Bound")
# st.plotly_chart(fig_fcst, use_container_width=True)
# 
# # -----------------------------
# # Relative Buzz Share (optional)
# # -----------------------------
# if show_share:
#     df_share = df.copy()
#     df_share[artists] = df_share[artists].div(df_share[artists].sum(axis=1), axis=0) * 100
#     fig_share = px.area(df_share, x="date", y=artists, title="Relative Buzz Share (%)")
#     st.plotly_chart(fig_share, use_container_width=True)
# 
# # -----------------------------
# # Insights Section
# # -----------------------------
# artist_insights = {
#     "Rihanna": "🔺 Consistent buzz with strong spikes around Fenty product launches and public appearances.",
#     "Taylor Swift": "🎤 Peaks during Eras Tour announcements, steady baseline growth across the year.",
#     "Blackpink": "🔥 Buzz cycles around comeback teasers, with sharp international spikes.",
#     "BTS": "📈 Attention surges with enlistment updates; fans sustain strong baseline buzz.",
#     # Add more insights as needed
# }
# 
# st.subheader(f"🔍 Insights — {artist}")
# st.write(artist_insights.get(artist, "No insights added yet."))
#

!streamlit run app.py --server.port 8501 &>/dev/null&

!pip install pyngrok

!ngrok config add-authtoken 32Mshd9EHuyYD2qpUmhTMHRBrWa_2VpcveFv9BkdnWo4ZagMH

from pyngrok import ngrok

ngrok.kill()

public_url = ngrok.connect(8501)
print("Streamlit app is live at:", public_url)