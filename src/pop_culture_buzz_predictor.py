# -*- coding: utf-8 -*-
"""Pop Culture Buzz Predictor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G2_N7p57WAIkqW4W9zJ-tTdW8U84OH7K
"""

!pip install pytrends prophet plotly pandas numpy matplotlib

import os
import time
import numpy as np
import pandas as pd
from pytrends.request import TrendReq

# -----------------------------
# Config
# -----------------------------
os.makedirs("data/raw", exist_ok=True)
pytrends = TrendReq(hl="en-US", tz=360)

anchor = "Instagram"  # a strong but not overwhelming anchor "Instagram" in this case
keywords = [
    'Rihanna', 'Kendrick Lamar', 'Taylor Swift', 'Travis Scott', 'BTS',
    'Blackpink', 'Doja Cat', 'Olivia Rodrigo', 'Zendaya', 'Jenna Ortega',
    'Wednesday', 'Stranger Things'
]

TIMEFRAME = "today 12-m"  # recent buzz
SLEEP_BETWEEN = 30        # giving time to gather data
RETRIES = 3

def fetch_pair_ratio(kw, anchor, retries=RETRIES, sleep_time=SLEEP_BETWEEN):
    """
    Fetch keyword+anchor from Google Trends and return a Series of
    100 * (kw / anchor) per date, with safe zero handling.
    """
    for attempt in range(1, retries+1):
        try:
            pytrends.build_payload([kw, anchor], timeframe=TIMEFRAME, geo="")
            df = pytrends.interest_over_time().drop(columns=["isPartial"], errors="ignore")
            if df.empty:
                print(f"Empty data for {kw}")
                return None

            # Per-date ratio: 100 * kw / anchor
            denom = df[anchor].replace(0, np.nan)
            ratio = 100.0 * (df[kw] / denom)

            # Handle NaNs from zero anchor: fill by interpolation, then zeros
            ratio = ratio.interpolate(limit_direction="both").fillna(0.0)

            ratio.name = kw
            return ratio

        except Exception as e:
            print(f"Error fetching {kw} (attempt {attempt}/{retries}): {e}")
            time.sleep(sleep_time * attempt)  # backoff
    return None

# -----------------------------
# Collect all series
# -----------------------------
series_list = []
for kw in keywords:
    print(f"\nFetching {kw} + {anchor} ({TIMEFRAME})")
    s = fetch_pair_ratio(kw, anchor)
    if s is not None:
        series_list.append(s)
    time.sleep(SLEEP_BETWEEN)

if not series_list:
    raise SystemExit("No data collected. Try increasing SLEEP_BETWEEN or changing TIMEFRAME/anchor.")

# Align on dates and combine
combined = pd.concat(series_list, axis=1)
combined.index.name = "date"

# -----------------------------
# Global rescale to 0â€“100
# -----------------------------
max_val = combined.to_numpy().max()
if max_val and max_val > 0:
    scale = 100.0 / max_val
    combined_scaled = combined * scale
else:
    combined_scaled = combined.copy() #safety net

# Save
out_path = "data/raw/google_trends_anchor_scaled.csv"
combined_scaled.reset_index().to_csv(out_path, index=False)
print(f"\n Saved comparable series to: {out_path}")
print("Shape:", combined_scaled.shape)
print(combined_scaled.head())

import pandas as pd
import numpy as np

df = pd.read_csv("google_trends_anchor_scaled.csv", parse_dates=["date"])

# Clean: replace zeros with NaN, interpolate, smooth with 7-day rolling mean
df_clean = (
    df.set_index("date")
      .replace(0, np.nan)
      .interpolate(method="linear", limit_direction="both")
      .rolling(window=7, min_periods=1)
      .mean()
      .reset_index()
)

out_path = "google_trends_clean.csv"
df_clean.to_csv(out_path, index=False)

print(f"Cleaned data saved: {out_path}")
print("Shape:", df_clean.shape)
print(df_clean.head())

import pandas as pd
import numpy as np

# -----------------------------
# Loading and preprocessing
# -----------------------------
df = pd.read_csv("google_trends_anchor_scaled.csv")

df['date'] = pd.to_datetime(df['date'], errors='coerce')
df = df.dropna(subset=['date']).sort_values('date').drop_duplicates('date')
df.set_index('date', inplace=True)

df.replace(0, np.nan, inplace=True)
df.dropna(axis=1, how='all', inplace=True)

# Convert index to numeric days for spline
df_numeric = df.copy()
df_numeric.index = (df_numeric.index - df_numeric.index[0]).days

# -----------------------------
# Interpolation
# -----------------------------
df_interp = pd.DataFrame(index=df_numeric.index)
for col in df_numeric.columns:
    valid_points = df_numeric[col].dropna()
    if len(valid_points) >= 3:
        df_interp[col] = df_numeric[col].interpolate(method='spline', order=2, limit_direction='both')
    else:
        df_interp[col] = df_numeric[col].interpolate(method='linear', limit_direction='both')

# Restore datetime index
df_interp.index = df.index
df_interp.fillna(method='ffill', inplace=True)
df_interp.fillna(method='bfill', inplace=True)

# -----------------------------
# Short vs. long smoothing
# -----------------------------
df_short = df_interp.rolling(window=14, min_periods=1).mean()
df_long  = df_interp.rolling(window=49, min_periods=1).mean()

# -----------------------------
# Spike detection
# -----------------------------
spikes = df_short - df_long
spike_flags = spikes.gt(spikes.mean() + 2*spikes.std())

# -----------------------------
# Save outputs
# -----------------------------
df_short.reset_index().to_csv("google_trends_short_term.csv", index=False)
df_long.reset_index().to_csv("google_trends_long_term.csv", index=False)
spikes.reset_index().to_csv("google_trends_spikes.csv", index=False)
spike_flags.reset_index().to_csv("google_trends_spike_flags.csv", index=False)

print("Short-term, long-term, spikes, and spike flags saved successfully!")

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.backends.backend_pdf import PdfPages

# Make sure docs folder exists
os.makedirs("docs", exist_ok=True)

# -----------------------------
# Load cleaned data
# -----------------------------
df_clean = pd.read_csv("google_trends_clean.csv", parse_dates=["date"])
df_clean['date'] = pd.to_datetime(df_clean['date'], errors='coerce')
df_clean = df_clean.dropna(subset=['date']).sort_values('date')
df_clean.set_index('date', inplace=True)
df_clean = df_clean.apply(pd.to_numeric, errors='coerce').replace(0, np.nan)

# Weekly resample
df_weekly = df_clean.resample('W').mean().ffill().bfill()

# Short vs. long smoothing
df_short = df_clean.rolling(window=14, min_periods=1).mean()
df_long  = df_clean.rolling(window=49, min_periods=1).mean()

# -----------------------------
# Export all plots to PDF
# -----------------------------
with PdfPages("docs/buzz_trends_report.pdf") as pdf:

    # --- Plot 1: Short vs Long buzz ---
    plt.figure(figsize=(18,8))
    for col in df_short.columns:
        plt.plot(df_short.index, df_short[col], label=f"{col} (short-term)")
        plt.plot(df_long.index, df_long[col], linestyle='--', alpha=0.7, label=f"{col} (long-term)")
    plt.title("Buzz Over Time (Short-term vs Long-term)", fontsize=16)
    plt.xlabel("Date", fontsize=14); plt.ylabel("Buzz (Scaled)", fontsize=14)
    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=10, ncol=2)
    plt.grid(alpha=0.3); plt.tight_layout()
    pdf.savefig(); plt.close()

    # --- Plot 2: Weekly Buzz ---
    plt.figure(figsize=(18,8))
    for col in df_weekly.columns:
        plt.plot(df_weekly.index, df_weekly[col], label=col)
    plt.title("Weekly Buzz Over Time", fontsize=16)
    plt.xlabel("Week", fontsize=14); plt.ylabel("Buzz (Scaled)", fontsize=14)
    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=10, ncol=2)
    plt.grid(alpha=0.3); plt.tight_layout()
    pdf.savefig(); plt.close()

    # --- Plot 3: Correlation Heatmap ---
    corr = df_weekly.corr()
    plt.figure(figsize=(12,10))
    sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", cbar=True, square=True, linewidths=0.5)
    plt.title("Correlation Heatmap of Weekly Buzz", fontsize=16)
    plt.tight_layout()
    pdf.savefig(); plt.close()

    # --- Plot 4: Relative Buzz Share ---
    df_relative = df_weekly.div(df_weekly.sum(axis=1), axis=0) * 100
    plt.figure(figsize=(14,7))
    df_relative.plot.area(alpha=0.7, figsize=(14,7))
    plt.title("Relative Buzz Share Over Time (Weekly, %)", fontsize=16)
    plt.ylabel("Relative Share (%)"); plt.xlabel("Week")
    plt.legend(title="Keywords", bbox_to_anchor=(1.05, 1), loc="upper left")
    plt.tight_layout()
    pdf.savefig(); plt.close()

print(" All plots saved to docs/buzz_trends_report.pdf")

# -----------------------------
# Show only the last 2 plots inline
# -----------------------------

# Correlation Heatmap
plt.figure(figsize=(12,10))
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", cbar=True, square=True, linewidths=0.5)
plt.title("Correlation Heatmap of Weekly Buzz", fontsize=16)
plt.tight_layout()
plt.show()

# Relative Buzz Share
plt.figure(figsize=(14,7))
df_relative.plot.area(alpha=0.7, figsize=(14,7))
plt.title("Relative Buzz Share Over Time (Weekly, %)", fontsize=16)
plt.ylabel("Relative Share (%)"); plt.xlabel("Week")
plt.legend(title="Keywords", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.show()

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.backends.backend_pdf import PdfPages
import matplotlib.cm as cm

# -----------------------------
# Ensure output folder exists
# -----------------------------
os.makedirs("docs", exist_ok=True)

# -----------------------------
# Load cleaned data
# -----------------------------
df_clean = pd.read_csv("google_trends_clean.csv", parse_dates=["date"])
df_clean['date'] = pd.to_datetime(df_clean['date'], errors='coerce')
df_clean = df_clean.dropna(subset=['date']).sort_values('date')
df_clean.set_index('date', inplace=True)
df_clean = df_clean.apply(pd.to_numeric, errors='coerce').replace(0, np.nan)

# Weekly resample
df_weekly = df_clean.resample('W').mean().ffill().bfill()

# Short vs. long smoothing
df_short = df_clean.rolling(window=14, min_periods=1).mean()
df_long  = df_clean.rolling(window=49, min_periods=1).mean()

# Relative buzz
df_relative = df_weekly.div(df_weekly.sum(axis=1), axis=0) * 100

# -----------------------------
# Load spike flags
# -----------------------------
df_spikes = pd.read_csv("google_trends_spike_flags.csv", parse_dates=["date"])
df_spikes.set_index("date", inplace=True)
n_spikes = len(df_spikes.columns)
colors = cm.get_cmap("tab10", n_spikes)  # distinct colors

# -----------------------------
# Export all plots to PDF
# -----------------------------
with PdfPages("docs/buzz_trends_report.pdf") as pdf:

    # --- Plot 1: Short vs Long buzz ---
    plt.figure(figsize=(18,8))
    for col in df_short.columns:
        plt.plot(df_short.index, df_short[col], label=f"{col} (short-term)")
        plt.plot(df_long.index, df_long[col], linestyle='--', alpha=0.7, label=f"{col} (long-term)")
    plt.title("Buzz Over Time (Short-term vs Long-term)", fontsize=16)
    plt.xlabel("Date", fontsize=14); plt.ylabel("Buzz (Scaled)", fontsize=14)
    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=10, ncol=2)
    plt.grid(alpha=0.3); plt.tight_layout()
    pdf.savefig(); plt.close()

    # --- Plot 2: Weekly Buzz ---
    plt.figure(figsize=(18,8))
    for col in df_weekly.columns:
        plt.plot(df_weekly.index, df_weekly[col], label=col)
    plt.title("Weekly Buzz Over Time", fontsize=16)
    plt.xlabel("Week", fontsize=14); plt.ylabel("Buzz (Scaled)", fontsize=14)
    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=10, ncol=2)
    plt.grid(alpha=0.3); plt.tight_layout()
    pdf.savefig(); plt.close()

    # --- Plot 3: Correlation Heatmap ---
    corr = df_weekly.corr()
    plt.figure(figsize=(12,10))
    sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", cbar=True, square=True, linewidths=0.5)
    plt.title("Correlation Heatmap of Weekly Buzz", fontsize=16)
    plt.tight_layout()
    pdf.savefig(); plt.close()

    # --- Plot 4: Relative Buzz Share ---
    plt.figure(figsize=(14,7))
    df_relative.plot.area(alpha=0.7, figsize=(14,7))
    plt.title("Relative Buzz Share Over Time (Weekly, %)", fontsize=16)
    plt.ylabel("Relative Share (%)"); plt.xlabel("Week")
    plt.legend(title="Keywords", bbox_to_anchor=(1.05, 1), loc="upper left")
    plt.tight_layout()
    pdf.savefig(); plt.close()

    # --- Plot 5: Buzz Spike Events ---
    plt.figure(figsize=(12,6))
    for i, col in enumerate(df_spikes.columns):
        spike_dates = df_spikes.index[df_spikes[col] == True]
        plt.scatter(spike_dates, [col]*len(spike_dates),
                    marker="*", s=150, color=colors(i), label=col)
    plt.title("Buzz Spike Events (âš¡)", fontsize=14)
    plt.xlabel("Date")
    plt.ylabel("Keyword / Artist")
    plt.xticks(rotation=45)
    plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
    plt.tight_layout()
    pdf.savefig(); plt.close()

print("All 5 plots saved to docs/buzz_trends_report.pdf")

# -----------------------------
# Show only the last 3 plots inline
# -----------------------------

# Correlation Heatmap
plt.figure(figsize=(12,10))
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", cbar=True, square=True, linewidths=0.5)
plt.title("Correlation Heatmap of Weekly Buzz", fontsize=16)
plt.tight_layout()
plt.show()

# Relative Buzz Share
plt.figure(figsize=(14,7))
df_relative.plot.area(alpha=0.7, figsize=(14,7))
plt.title("Relative Buzz Share Over Time (Weekly, %)", fontsize=16)
plt.ylabel("Relative Share (%)"); plt.xlabel("Week")
plt.legend(title="Keywords", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.show()

# Buzz Spike Events
plt.figure(figsize=(12,6))
for i, col in enumerate(df_spikes.columns):
    spike_dates = df_spikes.index[df_spikes[col] == True]
    plt.scatter(spike_dates, [col]*len(spike_dates),
                marker="*", s=150, color=colors(i), label=col)
plt.title("Buzz Spike Events (âš¡)", fontsize=14)
plt.xlabel("Date")
plt.ylabel("Keyword / Artist")
plt.xticks(rotation=45)
plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.show()

!pip install prophet pandas matplotlib

import os
import pandas as pd
import matplotlib.pyplot as plt
from prophet import Prophet

# -----------------------------
# Config
# -----------------------------
CLEAN_CSV = "google_trends_clean.csv"
FORECAST_WEEKS = 12
OUT_DIR = "forecasts_prophet"
os.makedirs(OUT_DIR, exist_ok=True)

# -----------------------------
# Load + prepare weekly data
# -----------------------------
df = pd.read_csv(CLEAN_CSV)
df["date"] = pd.to_datetime(df["date"], errors="coerce")
df = df.dropna(subset=["date"]).set_index("date")
df = df.apply(pd.to_numeric, errors="coerce")
dfW = df.resample("W").mean().ffill().bfill()
print("Data prepared:", dfW.shape)

# -----------------------------
# Forecast function
# -----------------------------
def forecast_artist(series_name, horizon=FORECAST_WEEKS):
    s = dfW[[series_name]].rename(columns={series_name: "y"})
    s["ds"] = s.index
    s["y"] = s["y"].clip(lower=0)

    m = Prophet(weekly_seasonality=True, yearly_seasonality=False, daily_seasonality=False, changepoint_prior_scale=0.5)
    m.fit(s)

    future = m.make_future_dataframe(periods=horizon, freq="W")
    fcst = m.predict(future)

    # Save CSV
    out_csv = os.path.join(OUT_DIR, f"{series_name.replace(' ', '_')}_forecast.csv")
    fcst[["ds", "yhat", "yhat_lower", "yhat_upper"]].to_csv(out_csv, index=False)

    # Save plot
    plt.figure(figsize=(12,6))
    plt.plot(s["ds"], s["y"], label="history")
    plt.plot(fcst["ds"], fcst["yhat"], linestyle="--", label="forecast")
    plt.fill_between(fcst["ds"], fcst["yhat_lower"], fcst["yhat_upper"], alpha=0.2)
    plt.title(f"{series_name} â€” {horizon}-week Forecast")
    plt.xlabel("Week"); plt.ylabel("Buzz (scaled)")
    plt.legend(); plt.grid(alpha=0.3); plt.tight_layout()
    plt.savefig(os.path.join(OUT_DIR, f"{series_name.replace(' ', '_')}_forecast.png"), dpi=150)
    plt.close()

    print(f"{series_name}: forecast saved")
    return fcst

# -----------------------------
# Run forecasts for all keywords/artists
# -----------------------------
all_forecasts = {col: forecast_artist(col) for col in dfW.columns}
print(f"\nForecasting complete. Results in: {OUT_DIR}")

from prophet import Prophet
import pandas as pd
import matplotlib.pyplot as plt

# -----------------------------
# Manual backtest with MAPE
# -----------------------------
def manual_backtest(series_name, train_frac=0.8):
    """
    Train Prophet on first (train_frac) of data,
    test on the rest, return MAPE %.
    """
    s = dfW[[series_name]].rename(columns={series_name: "y"})
    s["ds"] = s.index
    s["y"] = s["y"].clip(lower=1e-3)  # avoid zeros

    n_train = int(len(s) * train_frac)
    train, test = s.iloc[:n_train], s.iloc[n_train:]

    # Fit Prophet
    m = Prophet(
        weekly_seasonality=True,
        yearly_seasonality=False,
        daily_seasonality=False,
        interval_width=0.8
    )
    m.fit(train)

    # Forecast length = test size
    future = m.make_future_dataframe(periods=len(test), freq="W")
    forecast = m.predict(future)

    # Merge true vs predicted
    merged = test.merge(forecast[["ds", "yhat"]], on="ds", how="left")

    # Compute MAPE
    mape = (abs((merged["y"] - merged["yhat"]) / merged["y"]).mean()) * 100
    return mape

# -----------------------------
# Run evaluation for all artists
# -----------------------------
mape_scores = {col: manual_backtest(col) for col in dfW.columns}

# Sort ascending (best first)
mape_scores = dict(sorted(mape_scores.items(), key=lambda x: x[1]))

# Save results
pd.Series(mape_scores, name="MAPE (%)").to_csv("forecasts_prophet/mape_scores_manual.csv")

# -----------------------------
# Bar chart of MAPE per artist
# -----------------------------
plt.figure(figsize=(10,6))
plt.bar(mape_scores.keys(), mape_scores.values(), color="skyblue")
plt.xticks(rotation=45, ha="right")
plt.ylabel("MAPE (%)")
plt.title("Manual Backtest Forecast Error (MAPE) per Artist")
plt.tight_layout()
plt.savefig("forecasts_prophet/mape_bar_chart_manual.png", dpi=150)
plt.show()

print("Manual backtest complete. Results saved to forecasts_prophet/mape_scores_manual.csv and mape_bar_chart_manual.png")